{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "news = fetch_20newsgroups(subset='all')\n",
    "X, y = news.data , news.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk ,re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"From: Mamatha Devineni Ratnam <mr47+@andrew.cmu.edu>\\nSubject: Pens fans reactions\\nOrganization: Post Office, Carnegie Mellon, Pittsburgh, PA\\nLines: 12\\nNNTP-Posting-Host: po4.andrew.cmu.edu\\n\\n\\n\\nI am sure some bashers of Pens fans are pretty confused about the lack\\nof any kind of posts about the recent Pens massacre of the Devils. Actually,\\nI am  bit puzzled too and a bit relieved. However, I am going to put an end\\nto non-PIttsburghers' relief with a bit of praise for the Pens. Man, they\\nare killing those Devils worse than I thought. Jagr just showed you why\\nhe is much better than his regular season stats. He is also a lot\\nfo fun to watch in the playoffs. Bowman should let JAgr have a lot of\\nfun in the next couple of games since the Pens are going to beat the pulp out of Jersey anyway. I was very disappointed not to see the Islanders lose the final\\nregular season game.          PENS RULE!!!\\n\\n\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def news_to_sentences(news):\n",
    "    news_text = BeautifulSoup(news).get_text()\n",
    "    \n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    raw_sentences = tokenizer.tokenize(news_text)\n",
    "    \n",
    "    sentences = []\n",
    "    \n",
    "    for sent in raw_sentences:\n",
    "        sentences.append(re.sub('[^a-zA-Z]', ' ', sent.lower().strip()).split())\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\RUANJIAN\\Anaconda3\\Anaconda3_3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 184 of the file D:\\RUANJIAN\\Anaconda3\\Anaconda3_3\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "\n",
    "for x in X:\n",
    "    sentences += news_to_sentences(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"From: Mamatha Devineni Ratnam <mr47+@andrew.cmu.edu>\\nSubject: Pens fans reactions\\nOrganization: Post Office, Carnegie Mellon, Pittsburgh, PA\\nLines: 12\\nNNTP-Posting-Host: po4.andrew.cmu.edu\\n\\n\\n\\nI am sure some bashers of Pens fans are pretty confused about the lack\\nof any kind of posts about the recent Pens massacre of the Devils. Actually,\\nI am  bit puzzled too and a bit relieved. However, I am going to put an end\\nto non-PIttsburghers' relief with a bit of praise for the Pens. Man, they\\nare killing those Devils worse than I thought. Jagr just showed you why\\nhe is much better than his regular season stats. He is also a lot\\nfo fun to watch in the playoffs. Bowman should let JAgr have a lot of\\nfun in the next couple of games since the Pens are going to beat the pulp out of Jersey anyway. I was very disappointed not to see the Islanders lose the final\\nregular season game.          PENS RULE!!!\\n\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(303560, 18846)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences) , len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\RUANJIAN\\Anaconda3\\Anaconda3_3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 184 of the file D:\\RUANJIAN\\Anaconda3\\Anaconda3_3\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"From: Mamatha Devineni Ratnam \\nSubject: Pens fans reactions\\nOrganization: Post Office, Carnegie Mellon, Pittsburgh, PA\\nLines: 12\\nNNTP-Posting-Host: po4.andrew.cmu.edu\\n\\n\\n\\nI am sure some bashers of Pens fans are pretty confused about the lack\\nof any kind of posts about the recent Pens massacre of the Devils. Actually,\\nI am  bit puzzled too and a bit relieved. However, I am going to put an end\\nto non-PIttsburghers' relief with a bit of praise for the Pens. Man, they\\nare killing those Devils worse than I thought. Jagr just showed you why\\nhe is much better than his regular season stats. He is also a lot\\nfo fun to watch in the playoffs. Bowman should let JAgr have a lot of\\nfun in the next couple of games since the Pens are going to beat the pulp out of Jersey anyway. I was very disappointed not to see the Islanders lose the final\\nregular season game.          PENS RULE!!!\\n\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BeautifulSoup(X[0]).get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "902"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"From: Mamatha Devineni Ratnam <mr47+@andrew.cmu.edu>\\nSubject: Pens fans reactions\\nOrganization: Post Office, Carnegie Mellon, Pittsburgh, PA\\nLines: 12\\nNNTP-Posting-Host: po4.andrew.cmu.edu\\n\\n\\n\\nI am sure some bashers of Pens fans are pretty confused about the lack\\nof any kind of posts about the recent Pens massacre of the Devils. Actually,\\nI am  bit puzzled too and a bit relieved. However, I am going to put an end\\nto non-PIttsburghers' relief with a bit of praise for the Pens. Man, they\\nare killing those Devils worse than I thought. Jagr just showed you why\\nhe is much better than his regular season stats. He is also a lot\\nfo fun to watch in the playoffs. Bowman should let JAgr have a lot of\\nfun in the next couple of games since the Pens are going to beat the pulp out of Jersey anyway. I was very disappointed not to see the Islanders lose the final\\nregular season game.          PENS RULE!!!\\n\\n\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def review_to_wordlist(review):\n",
    "    '''\n",
    "    把IMDB的评论转成词序列\n",
    "    参考：http://blog.csdn.net/longxinchen_ml/article/details/50629613\n",
    "    '''\n",
    "    # 去掉HTML标签，拿到内容\n",
    "    review_text = BeautifulSoup(review).get_text()\n",
    "    # 用正则表达式取出符合规范的部分\n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
    "    # 小写化所有的词，并转成词list\n",
    "    words = review_text.lower().split()\n",
    "    # 返回words\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\RUANJIAN\\Anaconda3\\Anaconda3_3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 184 of the file D:\\RUANJIAN\\Anaconda3\\Anaconda3_3\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(902, 880, 161)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X[0]) , len(BeautifulSoup(X[0]).get_text()) ,len(re.sub(\"[^a-zA-Z]\",\" \", BeautifulSoup(X[0]).get_text()).lower().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\RUANJIAN\\Anaconda3\\Anaconda3_3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 184 of the file D:\\RUANJIAN\\Anaconda3\\Anaconda3_3\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'From  Mamatha Devineni Ratnam  Subject  Pens fans reactions Organization  Post Office  Carnegie Mellon  Pittsburgh  PA Lines     NNTP Posting Host  po  andrew cmu edu    I am sure some bashers of Pens fans are pretty confused about the lack of any kind of posts about the recent Pens massacre of the Devils  Actually  I am  bit puzzled too and a bit relieved  However  I am going to put an end to non PIttsburghers  relief with a bit of praise for the Pens  Man  they are killing those Devils worse than I thought  Jagr just showed you why he is much better than his regular season stats  He is also a lot fo fun to watch in the playoffs  Bowman should let JAgr have a lot of fun in the next couple of games since the Pens are going to beat the pulp out of Jersey anyway  I was very disappointed not to see the Islanders lose the final regular season game           PENS RULE     '"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(\"[^a-zA-Z]\",\" \", BeautifulSoup(X[0]).get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\RUANJIAN\\Anaconda3\\Anaconda3_3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 184 of the file D:\\RUANJIAN\\Anaconda3\\Anaconda3_3\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "161"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences1 = review_to_wordlist(X[0])\n",
    "len(sentences1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['from',\n",
       " 'mamatha',\n",
       " 'devineni',\n",
       " 'ratnam',\n",
       " 'subject',\n",
       " 'pens',\n",
       " 'fans',\n",
       " 'reactions',\n",
       " 'organization',\n",
       " 'post',\n",
       " 'office',\n",
       " 'carnegie',\n",
       " 'mellon',\n",
       " 'pittsburgh',\n",
       " 'pa',\n",
       " 'lines',\n",
       " 'nntp',\n",
       " 'posting',\n",
       " 'host',\n",
       " 'po',\n",
       " 'andrew',\n",
       " 'cmu',\n",
       " 'edu',\n",
       " 'i',\n",
       " 'am',\n",
       " 'sure',\n",
       " 'some',\n",
       " 'bashers',\n",
       " 'of',\n",
       " 'pens',\n",
       " 'fans',\n",
       " 'are',\n",
       " 'pretty',\n",
       " 'confused',\n",
       " 'about',\n",
       " 'the',\n",
       " 'lack',\n",
       " 'of',\n",
       " 'any',\n",
       " 'kind',\n",
       " 'of',\n",
       " 'posts',\n",
       " 'about',\n",
       " 'the',\n",
       " 'recent',\n",
       " 'pens',\n",
       " 'massacre',\n",
       " 'of',\n",
       " 'the',\n",
       " 'devils']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\RUANJIAN\\Anaconda3\\Anaconda3_3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 184 of the file D:\\RUANJIAN\\Anaconda3\\Anaconda3_3\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(news_to_sentences(X[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\RUANJIAN\\Anaconda3\\Anaconda3_3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 184 of the file D:\\RUANJIAN\\Anaconda3\\Anaconda3_3\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['from',\n",
       "  'mamatha',\n",
       "  'devineni',\n",
       "  'ratnam',\n",
       "  'subject',\n",
       "  'pens',\n",
       "  'fans',\n",
       "  'reactions',\n",
       "  'organization',\n",
       "  'post',\n",
       "  'office',\n",
       "  'carnegie',\n",
       "  'mellon',\n",
       "  'pittsburgh',\n",
       "  'pa',\n",
       "  'lines',\n",
       "  'nntp',\n",
       "  'posting',\n",
       "  'host',\n",
       "  'po',\n",
       "  'andrew',\n",
       "  'cmu',\n",
       "  'edu',\n",
       "  'i',\n",
       "  'am',\n",
       "  'sure',\n",
       "  'some',\n",
       "  'bashers',\n",
       "  'of',\n",
       "  'pens',\n",
       "  'fans',\n",
       "  'are',\n",
       "  'pretty',\n",
       "  'confused',\n",
       "  'about',\n",
       "  'the',\n",
       "  'lack',\n",
       "  'of',\n",
       "  'any',\n",
       "  'kind',\n",
       "  'of',\n",
       "  'posts',\n",
       "  'about',\n",
       "  'the',\n",
       "  'recent',\n",
       "  'pens',\n",
       "  'massacre',\n",
       "  'of',\n",
       "  'the',\n",
       "  'devils'],\n",
       " ['actually',\n",
       "  'i',\n",
       "  'am',\n",
       "  'bit',\n",
       "  'puzzled',\n",
       "  'too',\n",
       "  'and',\n",
       "  'a',\n",
       "  'bit',\n",
       "  'relieved'],\n",
       " ['however',\n",
       "  'i',\n",
       "  'am',\n",
       "  'going',\n",
       "  'to',\n",
       "  'put',\n",
       "  'an',\n",
       "  'end',\n",
       "  'to',\n",
       "  'non',\n",
       "  'pittsburghers',\n",
       "  'relief',\n",
       "  'with',\n",
       "  'a',\n",
       "  'bit',\n",
       "  'of',\n",
       "  'praise',\n",
       "  'for',\n",
       "  'the',\n",
       "  'pens'],\n",
       " ['man',\n",
       "  'they',\n",
       "  'are',\n",
       "  'killing',\n",
       "  'those',\n",
       "  'devils',\n",
       "  'worse',\n",
       "  'than',\n",
       "  'i',\n",
       "  'thought'],\n",
       " ['jagr',\n",
       "  'just',\n",
       "  'showed',\n",
       "  'you',\n",
       "  'why',\n",
       "  'he',\n",
       "  'is',\n",
       "  'much',\n",
       "  'better',\n",
       "  'than',\n",
       "  'his',\n",
       "  'regular',\n",
       "  'season',\n",
       "  'stats'],\n",
       " ['he',\n",
       "  'is',\n",
       "  'also',\n",
       "  'a',\n",
       "  'lot',\n",
       "  'fo',\n",
       "  'fun',\n",
       "  'to',\n",
       "  'watch',\n",
       "  'in',\n",
       "  'the',\n",
       "  'playoffs'],\n",
       " ['bowman',\n",
       "  'should',\n",
       "  'let',\n",
       "  'jagr',\n",
       "  'have',\n",
       "  'a',\n",
       "  'lot',\n",
       "  'of',\n",
       "  'fun',\n",
       "  'in',\n",
       "  'the',\n",
       "  'next',\n",
       "  'couple',\n",
       "  'of',\n",
       "  'games',\n",
       "  'since',\n",
       "  'the',\n",
       "  'pens',\n",
       "  'are',\n",
       "  'going',\n",
       "  'to',\n",
       "  'beat',\n",
       "  'the',\n",
       "  'pulp',\n",
       "  'out',\n",
       "  'of',\n",
       "  'jersey',\n",
       "  'anyway'],\n",
       " ['i',\n",
       "  'was',\n",
       "  'very',\n",
       "  'disappointed',\n",
       "  'not',\n",
       "  'to',\n",
       "  'see',\n",
       "  'the',\n",
       "  'islanders',\n",
       "  'lose',\n",
       "  'the',\n",
       "  'final',\n",
       "  'regular',\n",
       "  'season',\n",
       "  'game'],\n",
       " ['pens', 'rule'],\n",
       " []]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_to_sentences(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\RUANJIAN\\Anaconda3\\Anaconda3_3\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 20   # Minimum word count                        \n",
    "num_workers = 2     # Number of threads to run in parallel\n",
    "context = 5        # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "# Initialize and train the model (this will take some time)\n",
    "from gensim.models import word2vec\n",
    "\n",
    "model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling)\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\RUANJIAN\\Anaconda3\\Anaconda3_3\\lib\\site-packages\\ipykernel\\__main__.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('afternoon', 0.7894127368927002),\n",
       " ('weekend', 0.7645624876022339),\n",
       " ('evening', 0.7498670816421509),\n",
       " ('saturday', 0.7282308340072632),\n",
       " ('night', 0.7109854817390442),\n",
       " ('friday', 0.6805846691131592),\n",
       " ('newspaper', 0.6681157946586609),\n",
       " ('sunday', 0.6552820205688477),\n",
       " ('summer', 0.6469990015029907),\n",
       " ('week', 0.6440980434417725)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('morning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\RUANJIAN\\Anaconda3\\Anaconda3_3\\lib\\site-packages\\ipykernel\\__main__.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('she', 0.836227297782898),\n",
       " ('him', 0.6408571600914001),\n",
       " ('jesus', 0.6331659555435181),\n",
       " ('himself', 0.5874952673912048),\n",
       " ('his', 0.5843127965927124),\n",
       " ('nobody', 0.5719811916351318),\n",
       " ('her', 0.5678302645683289),\n",
       " ('it', 0.5555626749992371),\n",
       " ('god', 0.538629949092865),\n",
       " ('everybody', 0.5319458842277527)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('he')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\RUANJIAN\\Anaconda3\\Anaconda3_3\\lib\\site-packages\\ipykernel\\__main__.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-5.96275553e-02,  7.99844041e-02, -4.04534079e-02,  2.06259917e-03,\n",
       "        1.26356864e-02,  1.42318867e-02,  1.22728422e-02, -7.21804276e-02,\n",
       "        5.45303598e-02, -3.26478705e-02,  3.31209563e-02, -4.28872220e-02,\n",
       "        2.20825784e-02, -1.00719750e-01,  1.50049496e-02, -9.76634328e-04,\n",
       "       -1.07335769e-01, -5.05791605e-02,  8.78696442e-02, -4.52809297e-02,\n",
       "       -1.37747610e-02, -4.34161746e-04, -2.87661608e-02,  2.07833275e-02,\n",
       "       -4.90124710e-02,  1.37465131e-02, -1.54575137e-02,  1.07929613e-02,\n",
       "        2.83770971e-02,  2.28900127e-02, -1.82409305e-02, -6.88619688e-02,\n",
       "        7.84402564e-02, -8.17152485e-02,  1.23752765e-02, -3.35047022e-02,\n",
       "       -3.12486440e-02, -3.99231799e-02,  5.17948940e-02,  4.29590233e-02,\n",
       "        1.23568118e-01, -2.24949280e-03,  4.48698327e-02, -2.92056594e-02,\n",
       "        4.48198766e-02, -5.32044321e-02,  2.46899948e-02,  7.19618276e-02,\n",
       "        4.96463701e-02, -8.61510448e-03, -7.53626600e-03,  1.26553746e-02,\n",
       "        4.53562140e-02, -1.36981942e-02, -7.05249384e-02, -2.18742732e-02,\n",
       "       -4.75956872e-02,  2.49424148e-02,  1.53939088e-03,  1.50874294e-02,\n",
       "        2.86295894e-04,  7.51811042e-02, -5.56507111e-02, -1.60666520e-03,\n",
       "        9.76461358e-03, -8.34409595e-02, -1.39805954e-02, -4.01698835e-02,\n",
       "       -2.51753181e-02,  5.19457087e-02, -4.98395450e-02, -3.20558026e-02,\n",
       "       -8.16104636e-02, -3.01655345e-02, -6.25505671e-02, -2.07573595e-03,\n",
       "        6.50405511e-02,  1.99001208e-02,  1.38744591e-02,  5.91200963e-02,\n",
       "       -5.25538661e-02,  9.15592723e-03, -2.65470892e-02, -6.07497711e-03,\n",
       "        6.24290146e-02, -1.36508932e-02,  3.74969207e-02, -8.70247856e-02,\n",
       "       -2.65248660e-02, -1.11090438e-02, -1.50352921e-02, -2.48330757e-02,\n",
       "        8.07673857e-02,  7.85200670e-02,  3.85646150e-02,  4.22818251e-02,\n",
       "        1.03295073e-01,  4.03539604e-03,  2.83987424e-03,  6.63339198e-02,\n",
       "        1.15297385e-01, -1.03575975e-01, -2.88159102e-02, -3.21061574e-02,\n",
       "        1.61185060e-02,  7.38427192e-02, -6.44541392e-03,  8.29150062e-03,\n",
       "        1.27761960e-01, -1.30469441e-01,  1.47670299e-01, -5.32786362e-02,\n",
       "        1.04994774e-01, -3.72353606e-02,  1.01287119e-01,  7.95372762e-03,\n",
       "        1.28890425e-01,  8.48493427e-02, -2.42783632e-02,  4.63277325e-02,\n",
       "       -4.18128297e-02,  6.18297458e-02, -1.13992058e-02,  6.66815713e-02,\n",
       "        9.99823213e-02, -6.85494542e-02,  7.35919550e-02,  1.02765702e-01,\n",
       "       -1.56250782e-02,  1.05165407e-01,  1.49565618e-04,  5.85401729e-02,\n",
       "       -6.20822199e-02, -2.64434014e-02,  5.16470410e-02, -4.35619801e-02,\n",
       "        9.58618894e-02, -4.19117361e-02, -3.59663274e-03, -3.62229384e-02,\n",
       "       -3.53634823e-03, -4.22233865e-02,  1.04038611e-01,  3.59730348e-02,\n",
       "       -1.59024578e-02,  4.05470561e-03,  7.89316148e-02,  2.19096243e-03,\n",
       "        2.32217694e-03,  9.07451585e-02,  1.10225221e-02, -3.15929241e-02,\n",
       "       -8.37781932e-03, -9.64064598e-02, -1.24672301e-01, -1.38546070e-02,\n",
       "        3.97607498e-02,  7.53550455e-02, -1.21767975e-01,  1.22236103e-01,\n",
       "        1.23140942e-02, -7.55890310e-02, -4.69619744e-02,  6.87080622e-02,\n",
       "       -7.19777793e-02,  4.77084555e-02,  5.80831952e-02,  4.91964146e-02,\n",
       "        7.95511827e-02, -1.52125791e-01, -8.74936059e-02, -1.40027497e-02,\n",
       "        4.97549698e-02, -4.24322076e-02,  1.16130458e-02,  5.12751862e-02,\n",
       "       -7.37948343e-03, -1.05098739e-01, -8.80939141e-02,  7.05140233e-02,\n",
       "        3.45863439e-02,  4.03661393e-02, -4.54261862e-02,  9.91268829e-03,\n",
       "        3.40503152e-03, -5.38178384e-02, -8.38318393e-02, -3.54145318e-02,\n",
       "       -1.38565138e-01,  7.58452341e-03, -2.20824964e-03,  2.89564021e-02,\n",
       "       -7.94896781e-02,  5.30021675e-02, -3.96678224e-03,  3.79216485e-02,\n",
       "        5.64984791e-02,  2.16817427e-02, -5.49433567e-02,  3.67692672e-02,\n",
       "       -4.17142687e-03, -1.01539381e-01,  1.41188979e-01,  8.23256094e-03,\n",
       "        1.02424793e-01, -3.61586101e-02, -5.10618500e-02,  1.02333203e-01,\n",
       "        7.25948391e-03,  3.17912661e-02,  2.95518674e-02, -7.88601637e-02,\n",
       "        1.10765219e-01,  4.20864820e-02,  7.91356415e-02, -7.06770644e-02,\n",
       "        4.79876734e-02,  9.80931520e-02, -8.84733163e-03, -4.43804152e-02,\n",
       "       -6.15116023e-02,  5.35049774e-02,  3.51542532e-02, -1.70911495e-02,\n",
       "        4.58390266e-02,  4.46392298e-02,  7.13038966e-02, -1.39146904e-02,\n",
       "        4.63297628e-02,  1.96678769e-02,  9.32206437e-02, -2.69481931e-02,\n",
       "        8.93240720e-02,  4.70912270e-02, -5.43198995e-02,  1.72884651e-02,\n",
       "        1.04490826e-02,  4.16585691e-02, -9.95098203e-02, -7.18149543e-02,\n",
       "        1.19052334e-02, -1.76518336e-02,  4.87657897e-02,  4.00054753e-02,\n",
       "        9.67818201e-02,  7.35791102e-02, -6.33495441e-03,  6.03376096e-03,\n",
       "        3.45621780e-02, -1.36009362e-02,  2.25557163e-02, -1.66021008e-02,\n",
       "        2.42794100e-02, -3.06855571e-02, -9.32352990e-03, -5.06074354e-03,\n",
       "        1.03431977e-02, -9.30153355e-02, -2.85904165e-02,  3.11051239e-03,\n",
       "        9.30104926e-02, -9.25114229e-02,  3.31761837e-02, -2.72768289e-02,\n",
       "        1.18758842e-01, -1.13407865e-01, -1.24916621e-02, -9.63006616e-02,\n",
       "        2.99353078e-02, -5.15920250e-03,  1.51971104e-02,  1.98722463e-02,\n",
       "       -6.37508929e-02, -6.31013215e-02,  5.14228866e-02,  2.08957624e-02,\n",
       "        3.64618562e-03, -3.06588542e-02, -1.79629158e-02,  1.16514519e-03,\n",
       "        1.09357843e-02,  5.99325029e-03,  7.05115274e-02,  3.31860268e-03,\n",
       "        4.56059016e-02, -7.59727147e-04, -4.01284592e-03,  2.94093397e-02,\n",
       "        3.37057114e-02,  6.43138662e-02,  4.31269556e-02,  1.07068896e-01,\n",
       "       -4.90300991e-02, -4.32257280e-02, -8.34297314e-02, -5.52971996e-02,\n",
       "        4.18374836e-02, -6.03891797e-02, -4.84862104e-02, -5.25641292e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk.data\n",
    "\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def review_to_text(review, remove_stopwords):\n",
    "    raw_text = BeautifulSoup(review, 'html').get_text()\n",
    "    \n",
    "    letters = re.sub('[^a-zA-Z]', ' ', raw_text)\n",
    "    \n",
    "    words = letters.lower().split()\n",
    "    \n",
    "    if remove_stopwords:\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        words = [w for w in words if w not in stop_words]\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def review_to_sentences(review, tokenizer):\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    \n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            sentences.append(review_to_text(raw_sentence, False))\n",
    "    \n",
    "    return sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\RUANJIAN\\Anaconda3\\Anaconda3_3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 184 of the file D:\\RUANJIAN\\Anaconda3\\Anaconda3_3\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n",
      "D:\\RUANJIAN\\Anaconda3\\Anaconda3_3\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \"b'.'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "D:\\RUANJIAN\\Anaconda3\\Anaconda3_3\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \"b'...'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "D:\\RUANJIAN\\Anaconda3\\Anaconda3_3\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \"b'....'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "D:\\RUANJIAN\\Anaconda3\\Anaconda3_3\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \"b'.. .'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "D:\\RUANJIAN\\Anaconda3\\Anaconda3_3\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \"b'...................'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "D:\\RUANJIAN\\Anaconda3\\Anaconda3_3\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \"b'..................................'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "D:\\RUANJIAN\\Anaconda3\\Anaconda3_3\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \"b'....................................'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "D:\\RUANJIAN\\Anaconda3\\Anaconda3_3\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \"b'..'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "D:\\RUANJIAN\\Anaconda3\\Anaconda3_3\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \"b'... .. ...   ... .. ...   ... .. ...    ... .. ...   ... .. ...   ... .. ...'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "D:\\RUANJIAN\\Anaconda3\\Anaconda3_3\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \"b'...............'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "D:\\RUANJIAN\\Anaconda3\\Anaconda3_3\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \"b'/'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "D:\\RUANJIAN\\Anaconda3\\Anaconda3_3\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \"b'......'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n"
     ]
    }
   ],
   "source": [
    "corpora = []  \n",
    "    \n",
    "for review in X:\n",
    "    corpora += review_to_sentences(review, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 20   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "print (\"Training model...\")\n",
    "model = word2vec.Word2Vec(corpora, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling)\n",
    "\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "#model_name = \"./fetch_20newsgroups\"\n",
    "#model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\RUANJIAN\\Anaconda3\\Anaconda3_3\\lib\\site-packages\\ipykernel\\__main__.py:3: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('woman', 0.7219079732894897),\n",
       " ('lord', 0.6027616262435913),\n",
       " ('god', 0.5938579440116882),\n",
       " ('soul', 0.5931066274642944),\n",
       " ('son', 0.5881056785583496),\n",
       " ('prophet', 0.5865452289581299),\n",
       " ('himself', 0.5831429362297058),\n",
       " ('satan', 0.5810807943344116),\n",
       " ('eve', 0.5808910131454468),\n",
       " ('heaven', 0.5797165632247925)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec.load(\"./fetch_20newsgroups\")\n",
    "model.most_similar(\"man\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "from nltk.corpus import stopwords\n",
    "def makeFeatureVec(words, model, num_features):\n",
    "\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "\n",
    "    nwords = 0.\n",
    "\n",
    "    index2word_set = set(model.wv.index2word)\n",
    "\n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1.\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "            \n",
    "    featureVec = np.divide(featureVec,nwords)\n",
    "    return featureVec\n",
    "\n",
    "\n",
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "\n",
    "    counter = 0\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "\n",
    "    for review in reviews:\n",
    "        reviewFeatureVecs[counter] = makeFeatureVec(review, model, num_features)\n",
    "        \n",
    "        counter += 1\n",
    "\n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\RUANJIAN\\Anaconda3\\Anaconda3_3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 184 of the file D:\\RUANJIAN\\Anaconda3\\Anaconda3_3\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n",
      "D:\\RUANJIAN\\Anaconda3\\Anaconda3_3\\lib\\site-packages\\ipykernel\\__main__.py:14: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "clean_train_reviews = []\n",
    "for review in X:\n",
    "    clean_train_reviews.append( review_to_text( review, remove_stopwords=True ))\n",
    "\n",
    "trainDataVecs = getAvgFeatureVecs( clean_train_reviews, model, num_features )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\RUANJIAN\\Anaconda3\\Anaconda3_3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "D:\\RUANJIAN\\Anaconda3\\Anaconda3_3\\lib\\site-packages\\sklearn\\grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 27 candidates, totalling 108 fits\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "gbc = GradientBoostingClassifier()\n",
    "\n",
    "params_gbc = {'n_estimators':[10, 100, 500], 'learning_rate':[0.01, 0.1, 1.0], 'max_depth': [2, 3, 4]}\n",
    "gs = GridSearchCV(gbc, params_gbc, cv=4, n_jobs=-1, verbose=1)\n",
    "\n",
    "gs.fit(trainDataVecs, y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gs.best_score_,gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = gs.predict(testDataVecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
