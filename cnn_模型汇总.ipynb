{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting E:\\python\\mnist_data\\train-images-idx3-ubyte.gz\n",
      "Extracting E:\\python\\mnist_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting E:\\python\\mnist_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting E:\\python\\mnist_data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(r'E:\\python\\mnist_data',one_hot=True)\n",
    "\n",
    "\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一种方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#占位符未知的数据类型\n",
    "#输入正确值,标签值(新的占位符)\n",
    "#这里的None表示此张量的第一个维度可以是任何长度的\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y_ = tf.placeholder(\"float\", [None,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W1 = tf.Variable(tf.zeros([784,500]))\n",
    "b1 = tf.Variable(tf.zeros([500]))\n",
    "#模型参数，可以用Variable表示\n",
    "W2 = tf.Variable(tf.zeros([500,10]))\n",
    "b2 = tf.Variable(tf.zeros([10]))\n",
    "#模型参数，可以用Variable表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a1 = tf.nn.softmax(tf.matmul(x,W1) + b1)\n",
    "y = tf.nn.softmax(tf.matmul(a1,W2) + b2)\n",
    "#用tf.matmul(​​X，W)表示x乘以W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y))\n",
    "#计算交叉熵\n",
    "train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n",
    "#梯度下降算法（gradient descent algorithm）以0.01的学习速率最小化交叉熵\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#保存模型设置\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "#初始化我们创建的变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#在训练之前，可以检查计算图 ，进行神经网络的可视化\n",
    "# 在 在logs 目录 下 输入 一下\n",
    "# tensorboard --logdir=./\n",
    "#浏览器输入 \n",
    "# localhost:6006\n",
    "merged = tf.summary.merge_all()\n",
    "writer = tf.summary.FileWriter(\"cnn/logs\", sess.graph)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#创建计算图\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "#在此时 ， 运行 sess可以检查 是否出错"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cnn/saver/moedl1.ckpt'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#在Session里面启动模型，并且初始化变量\n",
    "\n",
    "for i in range(100):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n",
    "saver.save(sess,'cnn/saver/moedl1.ckpt')\n",
    "#让模型循环训练1000次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " #让模型循环训练1000次\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "#测试真实标签匹配"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy 0.14\n"
     ]
    }
   ],
   "source": [
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "print (\"test accuracy %g\"%sess.run(accuracy, feed_dict={x: mnist.test.images[:100], y_: mnist.test.labels[:100]}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from cnn/saver/moedl1.ckpt\n"
     ]
    }
   ],
   "source": [
    "#神经网络训练完成，直接进行读取记录》\n",
    "saver = tf.train.Saver()\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(init)\n",
    "saver.restore(sess,'cnn/saver/moedl1.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy 0.14\n"
     ]
    }
   ],
   "source": [
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "print (\"test accuracy %g\"%sess.run(accuracy, feed_dict={x: mnist.test.images[:100], y_: mnist.test.labels[:100]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第二种 方法 使用cnn卷积神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting E:\\python\\mnist_data\\train-images-idx3-ubyte.gz\n",
      "Extracting E:\\python\\mnist_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting E:\\python\\mnist_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting E:\\python\\mnist_data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(r'E:\\python\\mnist_data', one_hot=True)\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_iters = 100\n",
    "batch_size = 20\n",
    "display_step = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Network Parameters\n",
    "n_input = 784 # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)\n",
    "dropout = 0.8 # Dropout, probability to keep units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tf Graph input\n",
    "x = tf.placeholder(tf.float32, [None, n_input])\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "keep_prob = tf.placeholder(tf.float32) # dropout (keep probability)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create custom model\n",
    "def conv2d(name, l_input, w, b):\n",
    "    return tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(l_input, w, strides=[1, 1, 1, 1], padding='SAME'),b), name=name)\n",
    "\n",
    "def max_pool(name, l_input, k):\n",
    "    return tf.nn.max_pool(l_input, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME', name=name)\n",
    "\n",
    "def norm(name, l_input, lsize=4):\n",
    "    return tf.nn.lrn(l_input, lsize, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name=name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def customnet(_X, _weights, _biases, _dropout):\n",
    "    # Reshape input picture\n",
    "    _X = tf.reshape(_X, shape=[-1, 28, 28, 1])\n",
    "\n",
    "    # Convolution Layer\n",
    "    conv1 = conv2d('conv1', _X, _weights['wc1'], _biases['bc1'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    pool1 = max_pool('pool1', conv1, k=2)\n",
    "    # Apply Normalization\n",
    "    norm1 = norm('norm1', pool1, lsize=4)\n",
    "    # Apply Dropout\n",
    "    norm1 = tf.nn.dropout(norm1, _dropout)\n",
    "\n",
    "    # Convolution Layer\n",
    "    conv2 = conv2d('conv2', norm1, _weights['wc2'], _biases['bc2'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    pool2 = max_pool('pool2', conv2, k=2)\n",
    "    # Apply Normalization\n",
    "    norm2 = norm('norm2', pool2, lsize=4)\n",
    "    # Apply Dropout\n",
    "    norm2 = tf.nn.dropout(norm2, _dropout)\n",
    "\n",
    "    # Convolution Layer\n",
    "    conv3 = conv2d('conv3', norm2, _weights['wc3'], _biases['bc3'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    pool3 = max_pool('pool3', conv3, k=2)\n",
    "    # Apply Normalization\n",
    "    norm3 = norm('norm3', pool3, lsize=4)\n",
    "    # Apply Dropout\n",
    "    norm3 = tf.nn.dropout(norm3, _dropout)\n",
    "    #conv4\n",
    "    conv4 = conv2d('conv4', norm3, _weights['wc4'], _biases['bc4'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    pool4 = max_pool('pool4', conv4, k=2)\n",
    "    # Apply Normalization\n",
    "    norm4 = norm('norm4', pool4, lsize=4)\n",
    "    # Apply Dropout\n",
    "    norm4 = tf.nn.dropout(norm4, _dropout)\n",
    "    # Fully connected layer\n",
    "    dense1 = tf.reshape(norm4, [-1, _weights['wd1'].get_shape().as_list()[0]]) # Reshape conv3 output to fit dense layer input\n",
    "    dense1 = tf.nn.relu(tf.matmul(dense1, _weights['wd1']) + _biases['bd1'], name='fc1') # Relu activation\n",
    "\n",
    "    dense2 = tf.nn.relu(tf.matmul(dense1, _weights['wd2']) + _biases['bd2'], name='fc2') # Relu activation\n",
    "\n",
    "    # Output, class prediction\n",
    "    out = tf.matmul(dense2, _weights['out']) + _biases['out']\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'wc1': tf.Variable(tf.random_normal([3, 3, 1, 64])),\n",
    "    'wc2': tf.Variable(tf.random_normal([3, 3, 64, 128])),\n",
    "    'wc3': tf.Variable(tf.random_normal([3, 3, 128, 256])),\n",
    "    'wc4': tf.Variable(tf.random_normal([2, 2, 256, 512])),\n",
    "    'wd1': tf.Variable(tf.random_normal([2*2*512, 1024])), \n",
    "    'wd2': tf.Variable(tf.random_normal([1024, 1024])),\n",
    "    'out': tf.Variable(tf.random_normal([1024, 10]))\n",
    "}\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.random_normal([64])),\n",
    "    'bc2': tf.Variable(tf.random_normal([128])),\n",
    "    'bc3': tf.Variable(tf.random_normal([256])),\n",
    "    'bc4': tf.Variable(tf.random_normal([512])),\n",
    "    'bd1': tf.Variable(tf.random_normal([1024])),\n",
    "    'bd2': tf.Variable(tf.random_normal([1024])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Construct model\n",
    "pred = customnet(x, weights, biases, keep_prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\RUANJIAN\\Anaconda3\\Anaconda3_3\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:107: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    }
   ],
   "source": [
    "# Initializing the variables\n",
    "init = tf.initialize_all_variables()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 200, Minibatch Loss= 292181.218750, Training Accuracy= 0.15000\n",
      "Iter 400, Minibatch Loss= 97581.484375, Training Accuracy= 0.25000\n",
      "Iter 600, Minibatch Loss= 206365.140625, Training Accuracy= 0.10000\n",
      "Iter 800, Minibatch Loss= 117109.265625, Training Accuracy= 0.10000\n",
      "Iter 1000, Minibatch Loss= 113146.765625, Training Accuracy= 0.15000\n",
      "Iter 1200, Minibatch Loss= 90711.625000, Training Accuracy= 0.10000\n",
      "Iter 1400, Minibatch Loss= 107905.976562, Training Accuracy= 0.25000\n",
      "Iter 1600, Minibatch Loss= 88965.390625, Training Accuracy= 0.10000\n",
      "Iter 1800, Minibatch Loss= 72404.687500, Training Accuracy= 0.35000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.1796875\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    # Keep training until reach max iterations\n",
    "    for step in range(1,100):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        # Fit training using batch data\n",
    "        sess.run(optimizer, feed_dict={x: batch_xs, y: batch_ys, keep_prob: dropout})\n",
    "        if step % 10 == 0:\n",
    "            # Calculate batch accuracy\n",
    "            acc = sess.run(accuracy, feed_dict={x: batch_xs, y: batch_ys, keep_prob: 1.})\n",
    "            # Calculate batch loss\n",
    "            loss = sess.run(cost, feed_dict={x: batch_xs, y: batch_ys, keep_prob: 1.})\n",
    "            print (\"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \"{:.5f}\".format(acc))\n",
    "            \n",
    "    print (\"Optimization Finished!\")\n",
    "    # Calculate accuracy for 256 mnist test images\n",
    "    print (\"Testing Accuracy:\", sess.run(accuracy, feed_dict={x: mnist.test.images[:256], y: mnist.test.labels[:256], keep_prob: 1.}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#第三种方法  inception_net 网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\RUANJIAN\\Anaconda3\\Anaconda3_3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting E:\\python\\mnist_data\\train-images-idx3-ubyte.gz\n",
      "Extracting E:\\python\\mnist_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting E:\\python\\mnist_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting E:\\python\\mnist_data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(r'E:\\python\\mnist_data', one_hot=True)\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_iters = 100\n",
    "batch_size = 20\n",
    "display_step = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Network Parameters\n",
    "n_input = 784 # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)\n",
    "dropout = 0.8 # Dropout, probability to keep units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tf Graph input\n",
    "x = tf.placeholder(tf.float32, [None, n_input])\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "keep_prob = tf.placeholder(tf.float32) # dropout (keep probability)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create custom model\n",
    "def conv2d(name, l_input, w, b):\n",
    "    return tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(l_input, w, strides=[1, 1, 1, 1], padding='SAME'),b), name=name)\n",
    "\n",
    "def max_pool(name, l_input, k):\n",
    "    return tf.nn.max_pool(l_input, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME', name=name)\n",
    "\n",
    "def norm(name, l_input, lsize=4):\n",
    "    return tf.nn.lrn(l_input, lsize, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name=name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'wa1': tf.Variable(tf.random_normal([1, 1, 1, 16])),\n",
    "    'wa3': tf.Variable(tf.random_normal([3, 3, 1, 16])),\n",
    "    'wa5': tf.Variable(tf.random_normal([5, 5, 1, 16])),\n",
    "    'wb1': tf.Variable(tf.random_normal([1, 1, 48, 64])),\n",
    "    'wb3': tf.Variable(tf.random_normal([3, 3, 48, 64])),\n",
    "    'wb5': tf.Variable(tf.random_normal([5, 5, 48, 64])),\n",
    "    'wd1': tf.Variable(tf.random_normal([7*7*192, 2048])), \n",
    "    'wd2': tf.Variable(tf.random_normal([2048, 1024])),\n",
    "    'out': tf.Variable(tf.random_normal([1024, 10]))\n",
    "}\n",
    "biases = {\n",
    "    'ba1': tf.Variable(tf.random_normal([16])),\n",
    "    'ba3': tf.Variable(tf.random_normal([16])),\n",
    "    'ba5': tf.Variable(tf.random_normal([16])),\n",
    "    'bb1': tf.Variable(tf.random_normal([64])),\n",
    "    'bb3': tf.Variable(tf.random_normal([64])),\n",
    "    'bb5': tf.Variable(tf.random_normal([64])),\n",
    "    'bd1': tf.Variable(tf.random_normal([2048])),\n",
    "    'bd2': tf.Variable(tf.random_normal([1024])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def inception_block_a(_X,_weights,_biases,_dropout):\n",
    "    \n",
    "    conv1 = conv2d('conv_a1', _X, _weights['wa1'], _biases['ba1'])\n",
    "    pool1 = max_pool('pool_a1', conv1, k=2)\n",
    "    norm1 = norm('norm1_a1', pool1, lsize=4)\n",
    "    norm1 = tf.nn.dropout(norm1, _dropout)\n",
    "    \n",
    "    conv3 = conv2d('conv_a3', _X, _weights['wa3'], _biases['ba3'])\n",
    "    pool3 = max_pool('pool_a3', conv3, k=2)\n",
    "    norm3 = norm('norm1_a3', pool3, lsize=4)\n",
    "    norm3 = tf.nn.dropout(norm3, _dropout)\n",
    "    \n",
    "    conv5 = conv2d('conv_a5', _X, _weights['wa5'], _biases['ba5'])\n",
    "    pool5 = max_pool('pool_a5', conv5, k=2)\n",
    "    norm5 = norm('norm1_a5', pool5, lsize=4)\n",
    "    norm5 = tf.nn.dropout(norm5, _dropout)\n",
    "    \n",
    "    inception = tf.concat([norm1,norm3,norm5],3)\n",
    "    return inception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inception_block_b(_X,_weights,_biases,_dropout):\n",
    "    \n",
    "    conv1 = conv2d('conv_b1', _X, _weights['wb1'], _biases['bb1'])\n",
    "    pool1 = max_pool('pool_b1', conv1, k=2)\n",
    "    norm1 = norm('norm1_b1', pool1, lsize=4)\n",
    "    norm1 = tf.nn.dropout(norm1, _dropout)\n",
    "    \n",
    "    conv3 = conv2d('conv_b3', _X, _weights['wb3'], _biases['bb3'])\n",
    "    pool3 = max_pool('pool_b3', conv3, k=2)\n",
    "    norm3 = norm('norm1_b3', pool3, lsize=4)\n",
    "    norm3 = tf.nn.dropout(norm3, _dropout)\n",
    "    \n",
    "    conv5 = conv2d('conv_b5', _X, _weights['wb5'], _biases['bb5'])\n",
    "    pool5 = max_pool('pool_b5', conv5, k=2)\n",
    "    norm5 = norm('norm1_b5', pool5, lsize=4)\n",
    "    norm5 = tf.nn.dropout(norm5, _dropout)\n",
    "    \n",
    "    inception = tf.concat([norm1,norm3,norm5],3)\n",
    "    return inception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def customnet(_X, _weights, _biases, _dropout):\n",
    "    _X = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
    "\n",
    "    a1 = inception_block_a(_X , weights,biases, _dropout)\n",
    "    \n",
    "    b = inception_block_b( a1 ,weights,biases, _dropout)\n",
    "    \n",
    "    dense1 = tf.reshape(b, [-1, _weights['wd1'].get_shape().as_list()[0]]) # Reshape conv3 output to fit dense layer input\n",
    "    \n",
    "    dense1 = tf.nn.relu(tf.matmul(dense1, _weights['wd1']) + _biases['bd1'], name='fc1') # Relu activation\n",
    "\n",
    "    dense2 = tf.nn.relu(tf.matmul(dense1, _weights['wd2']) + _biases['bd2'], name='fc2') # Relu activation\n",
    "\n",
    "    # Output, class prediction\n",
    "    out = tf.matmul(dense2, _weights['out']) + _biases['out']\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(10)])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = customnet(x, weights, biases, keep_prob)\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\RUANJIAN\\Anaconda3\\Anaconda3_3\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:107: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    }
   ],
   "source": [
    "# Initializing the variables\n",
    "init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 200, Minibatch Loss= 503426.406250, Training Accuracy= 0.10000\n",
      "Iter 400, Minibatch Loss= 245885.500000, Training Accuracy= 0.25000\n",
      "Iter 600, Minibatch Loss= 79074.640625, Training Accuracy= 0.45000\n",
      "Iter 800, Minibatch Loss= 135851.984375, Training Accuracy= 0.60000\n",
      "Iter 1000, Minibatch Loss= 60433.335938, Training Accuracy= 0.70000\n",
      "Iter 1200, Minibatch Loss= 51373.078125, Training Accuracy= 0.75000\n",
      "Iter 1400, Minibatch Loss= 118952.625000, Training Accuracy= 0.55000\n",
      "Iter 1600, Minibatch Loss= 55398.187500, Training Accuracy= 0.70000\n",
      "Iter 1800, Minibatch Loss= 24187.839844, Training Accuracy= 0.85000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.7265625\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    # Keep training until reach max iterations\n",
    "    for step in range(1,100):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        # Fit training using batch data\n",
    "        sess.run(optimizer, feed_dict={x: batch_xs, y: batch_ys, keep_prob: dropout})\n",
    "        if step % 10 == 0:\n",
    "            # Calculate batch accuracy\n",
    "            acc = sess.run(accuracy, feed_dict={x: batch_xs, y: batch_ys, keep_prob: 1.})\n",
    "            # Calculate batch loss\n",
    "            loss = sess.run(cost, feed_dict={x: batch_xs, y: batch_ys, keep_prob: 1.})\n",
    "            print (\"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \"{:.5f}\".format(acc))\n",
    "            \n",
    "    print (\"Optimization Finished!\")\n",
    "    # Calculate accuracy for 256 mnist test images\n",
    "    print (\"Testing Accuracy:\", sess.run(accuracy, feed_dict={x: mnist.test.images[:256], y: mnist.test.labels[:256], keep_prob: 1.}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#第四种方法 res_net 网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting E:\\python\\mnist_data\\train-images-idx3-ubyte.gz\n",
      "Extracting E:\\python\\mnist_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting E:\\python\\mnist_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting E:\\python\\mnist_data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(r'E:\\python\\mnist_data', one_hot=True)\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_iters = 100\n",
    "batch_size = 20\n",
    "display_step = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Network Parameters\n",
    "n_input = 784 # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)\n",
    "dropout = 0.8 # Dropout, probability to keep units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tf Graph input\n",
    "x = tf.placeholder(tf.float32, [None, n_input])\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "keep_prob = tf.placeholder(tf.float32) # dropout (keep probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create custom model\n",
    "def conv2d(name, l_input, w, b):\n",
    "    return tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(l_input, w, strides=[1, 1, 1, 1], padding='SAME'),b), name=name)\n",
    "\n",
    "def max_pool(name, l_input, k):\n",
    "    return tf.nn.max_pool(l_input, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME', name=name)\n",
    "\n",
    "def norm(name, l_input, lsize=4):\n",
    "    return tf.nn.lrn(l_input, lsize, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name=name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'wa': tf.Variable(tf.random_normal([3, 3, 1, 32])),\n",
    "    'wa1': tf.Variable(tf.random_normal([3, 3, 1, 32])),\n",
    "    'wa2': tf.Variable(tf.random_normal([3, 3, 32, 32])),\n",
    "    'wb': tf.Variable(tf.random_normal([3, 3, 32, 64])),\n",
    "    'wb1': tf.Variable(tf.random_normal([3, 3, 32, 64])),\n",
    "    'wb2': tf.Variable(tf.random_normal([3, 3, 64, 64])),\n",
    "    'wd1': tf.Variable(tf.random_normal([7*7*64, 1024])), \n",
    "    'wd2': tf.Variable(tf.random_normal([1024, 1024])),\n",
    "    'out': tf.Variable(tf.random_normal([1024, 10]))\n",
    "}\n",
    "biases = {\n",
    "    'ba': tf.Variable(tf.random_normal([32])),\n",
    "    'ba1': tf.Variable(tf.random_normal([32])),\n",
    "    'ba2': tf.Variable(tf.random_normal([32])),\n",
    "    'bb': tf.Variable(tf.random_normal([64])),\n",
    "    'bb1': tf.Variable(tf.random_normal([64])),\n",
    "    'bb2': tf.Variable(tf.random_normal([64])),\n",
    "    'bd1': tf.Variable(tf.random_normal([1024])),\n",
    "    'bd2': tf.Variable(tf.random_normal([1024])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def identity_block(_X, _weights , block, _biases,_dropout):\n",
    "    \n",
    "    x_shortcut = _X\n",
    "    \n",
    "    x_shortcut = tf.nn.bias_add(tf.nn.conv2d(x_shortcut, _weights['w'+block], strides=[1, 2, 2, 1], padding='SAME'),_biases['b'+block])\n",
    "    \n",
    "    conv1 = conv2d('conv_'+block+'1', _X, _weights['w'+block+'1'], _biases['b'+block+'1'])\n",
    "    pool1 = max_pool('pool_'+block+'1', conv1, k=2)\n",
    "    norm1 = norm('norm1_'+block+'1', pool1, lsize=1)\n",
    "    norm1 = tf.nn.dropout(norm1, _dropout)\n",
    "    \n",
    "    norm1 = tf.nn.bias_add(tf.nn.conv2d(norm1, _weights['w'+block+'2'], strides=[1, 1, 1, 1], padding='SAME'),_biases['b'+block+'2'])\n",
    "\n",
    "    norm2 = tf.add(x_shortcut ,  norm1 )\n",
    "    \n",
    "    norm2  = tf.nn.relu(norm2 ,  name='identity_block'+block)\n",
    "    \n",
    "    return norm2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def customnet(_X, _weights, _biases, _dropout):\n",
    "    _X = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
    "\n",
    "    a  = identity_block(_X, _weights , 'a', _biases, _dropout)\n",
    "    \n",
    "    b  = identity_block(a, _weights , 'b', _biases, _dropout)\n",
    "    \n",
    "    dense1 = tf.reshape(b, [-1, _weights['wd1'].get_shape().as_list()[0]]) # Reshape conv3 output to fit dense layer input\n",
    "    \n",
    "    dense1 = tf.nn.relu(tf.matmul(dense1, _weights['wd1']) + _biases['bd1'], name='fc1') # Relu activation\n",
    "\n",
    "    dense2 = tf.nn.relu(tf.matmul(dense1, _weights['wd2']) + _biases['bd2'], name='fc2') # Relu activation\n",
    "\n",
    "    # Output, class prediction\n",
    "    out = tf.matmul(dense2, _weights['out']) + _biases['out']\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(10)])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = customnet(x, weights, biases, keep_prob)\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\RUANJIAN\\Anaconda3\\Anaconda3_3\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:107: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    }
   ],
   "source": [
    "# Initializing the variables\n",
    "init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 200, Minibatch Loss= 8092278.500000, Training Accuracy= 0.15000\n",
      "Iter 400, Minibatch Loss= 6487982.500000, Training Accuracy= 0.10000\n",
      "Iter 600, Minibatch Loss= 2653633.250000, Training Accuracy= 0.40000\n",
      "Iter 800, Minibatch Loss= 3063494.000000, Training Accuracy= 0.40000\n",
      "Iter 1000, Minibatch Loss= 3200554.500000, Training Accuracy= 0.35000\n",
      "Iter 1200, Minibatch Loss= 3253205.500000, Training Accuracy= 0.40000\n",
      "Iter 1400, Minibatch Loss= 2544643.750000, Training Accuracy= 0.55000\n",
      "Iter 1600, Minibatch Loss= 2072073.375000, Training Accuracy= 0.55000\n",
      "Iter 1800, Minibatch Loss= 2728199.500000, Training Accuracy= 0.50000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.5859375\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    # Keep training until reach max iterations\n",
    "    for step in range(1,100):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        # Fit training using batch data\n",
    "        sess.run(optimizer, feed_dict={x: batch_xs, y: batch_ys, keep_prob: dropout})\n",
    "        if step % 10 == 0:\n",
    "            # Calculate batch accuracy\n",
    "            acc = sess.run(accuracy, feed_dict={x: batch_xs, y: batch_ys, keep_prob: 1.})\n",
    "            # Calculate batch loss\n",
    "            loss = sess.run(cost, feed_dict={x: batch_xs, y: batch_ys, keep_prob: 1.})\n",
    "            print (\"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \"{:.5f}\".format(acc))\n",
    "            \n",
    "    print (\"Optimization Finished!\")\n",
    "    # Calculate accuracy for 256 mnist test images\n",
    "    print (\"Testing Accuracy:\", sess.run(accuracy, feed_dict={x: mnist.test.images[:256], y: mnist.test.labels[:256], keep_prob: 1.}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第五种方法 卷积自动编码机"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(r'E:\\python\\mnist_data', one_hot=True)\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_iters = 100\n",
    "batch_size = 20\n",
    "display_step = 2\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 784 # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)\n",
    "dropout = 0.8 # Dropout, probability to keep units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tf Graph input\n",
    "x = tf.placeholder(tf.float32, [None, n_input])\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "keep_prob = tf.placeholder(tf.float32) # dropout (keep probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create custom model\n",
    "def conv2d(name, l_input, w, b):\n",
    "    return tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(l_input, w, strides=[1, 1, 1, 1], padding='SAME'),b), name=name)\n",
    "\n",
    "def max_pool(name, l_input, k):\n",
    "    return tf.nn.max_pool(l_input, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME', name=name)\n",
    "\n",
    "def norm(name, l_input, lsize=4):\n",
    "    return tf.nn.lrn(l_input, lsize, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name=name)\n",
    "\n",
    "def deconv2d(name, l_input, w, b,output_shape):\n",
    "    return tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d_transpose(l_input,w,output_shape=output_shape,strides=[1,2,2,1],padding=\"SAME\"),b), name=name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'wa1': tf.Variable(tf.random_normal([3, 3, 1, 32])),\n",
    "    'wa2': tf.Variable(tf.random_normal([3, 3, 32, 64])),\n",
    "    'wa3': tf.Variable(tf.random_normal([3, 3, 64, 128])),\n",
    "    'wb1': tf.Variable(tf.random_normal([3, 3, 64, 128])),\n",
    "    'wb2': tf.Variable(tf.random_normal([3, 3,32, 64])),\n",
    "    'wb3': tf.Variable(tf.random_normal([3, 3, 1, 32])),\n",
    "\n",
    "}\n",
    "biases = {\n",
    "    'ba1': tf.Variable(tf.random_normal([32])),\n",
    "    'ba2': tf.Variable(tf.random_normal([64])),\n",
    "    'ba3': tf.Variable(tf.random_normal([128])),\n",
    "    'bb1': tf.Variable(tf.random_normal([64])),\n",
    "    'bb2': tf.Variable(tf.random_normal([32])),\n",
    "    'bb3': tf.Variable(tf.random_normal([1])),\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def encoder(_X, _weights, _biases,_dropout):\n",
    "    \n",
    "    conv1 = conv2d('conv_a1', _X, _weights['wa1'], _biases['ba1'])\n",
    "    pool1 = max_pool('pool_a1', conv1, k=2)\n",
    "    norm1 = norm('norm1_a1', pool1, lsize=4)\n",
    "    norm1 = tf.nn.dropout(norm1, _dropout)\n",
    "    \n",
    "    conv2 = conv2d('conv_a1', norm1, _weights['wa2'], _biases['ba2'])\n",
    "    pool2 = max_pool('pool_a2', conv2, k=2)\n",
    "    norm2 = norm('norm1_a2', pool2, lsize=4)\n",
    "    norm2 = tf.nn.dropout(norm2, _dropout)\n",
    "    \n",
    "    conv3 = conv2d('conv_a3', norm2, _weights['wa3'], _biases['ba3'])\n",
    "    pool3 = max_pool('pool_a3', conv3, k=2)\n",
    "    norm3 = norm('norm1_a3', pool3, lsize=4)\n",
    "    norm3 = tf.nn.dropout(norm3, _dropout)\n",
    "    \n",
    "    return norm3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def decoder(_X, _weights, _biases,_dropout):\n",
    "    \n",
    "    conv1 = deconv2d('deconv_b1',_X, _weights['wb1'], _biases['bb1'],[batch_size,7,7,64])\n",
    "    norm1 = norm('norm1_b1', conv1, lsize=4)\n",
    "    norm1 = tf.nn.dropout(norm1, _dropout)\n",
    "    \n",
    "    conv2 = deconv2d('deconv_b2',norm1, _weights['wb2'], _biases['bb2'],[batch_size,14,14,32])\n",
    "    norm2 = norm('norm1_b2', conv2, lsize=4)\n",
    "    norm2 = tf.nn.dropout(norm2, _dropout)\n",
    "    \n",
    "    conv3 = deconv2d('deconv_b3',norm2, _weights['wb3'],  _biases['bb3'],[batch_size,28,28,1])\n",
    "    norm3 = norm('norm1_b3', conv3, lsize=4)\n",
    "    norm3 = tf.nn.dropout(norm3, _dropout)\n",
    "    \n",
    "    return norm3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def customnet(_X, _weights, _biases, _dropout):\n",
    "    \n",
    "    dense1 = tf.reshape(_X , [-1, _weights['wd1'].get_shape().as_list()[0]]) # Reshape conv3 output to fit dense layer input\n",
    "    \n",
    "    dense1 = tf.nn.relu(tf.matmul(dense1, _weights['wd1']) + _biases['bd1'], name='fc1') # Relu activation\n",
    "\n",
    "    dense2 = tf.nn.relu(tf.matmul(dense1, _weights['wd2']) + _biases['bd2'], name='fc2') # Relu activation\n",
    "\n",
    "    # Output, class prediction\n",
    "    out = tf.matmul(dense2, weights['out']) + biases['out']\n",
    "    \n",
    "    return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_X = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
    "en_data =encoder(_X, weights, biases,dropout)\n",
    "de_data = decoder(en_data, weights, biases,dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define loss and optimizer\n",
    "de_data = tf.reshape(de_data,[-1, n_input])\n",
    "en_loss = tf.reduce_mean(tf.pow(tf.subtract(de_data, x), 2.0)) \n",
    "en_optimizer = tf.train.AdamOptimizer(0.01).minimize(en_loss) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 200, Minibatch Loss= 312.750580\n",
      "Iter 400, Minibatch Loss= 27.594934\n",
      "Iter 600, Minibatch Loss= 4.931098\n",
      "Iter 800, Minibatch Loss= 2.536595\n",
      "Iter 1000, Minibatch Loss= 1.661268\n",
      "Iter 1200, Minibatch Loss= 0.927785\n",
      "Iter 1400, Minibatch Loss= 0.938704\n",
      "Iter 1600, Minibatch Loss= 0.559970\n",
      "Iter 1800, Minibatch Loss= 0.401739\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    # Keep training until reach max iterations\n",
    "    for step in range(1,100):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        # Fit training using batch data\n",
    "        sess.run(en_optimizer, feed_dict={x: batch_xs, keep_prob: dropout})\n",
    "        if step % 10 == 0:\n",
    "              # Calculate batch loss\n",
    "            loss = sess.run(en_loss, feed_dict={x: batch_xs, keep_prob: 1.})\n",
    "            print (\"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + \"{:.6f}\".format(loss))\n",
    "    saver.save(sess,'saver/CAE_moedl.ckpt')\n",
    "    print (\"Optimization Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_image(image):\n",
    "    plt.imshow(image.reshape((28,28)),interpolation='nearest',cmap='binary')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/T+1IwYJTxQbdQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x22b8e033fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from saver/CAE_moedl.ckpt\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nl5oyfqkp45eaMn6pKeOXmvp/ZAVWvVDGB7EAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x22b86e1a6a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reimg = mnist.train.images[0].reshape(28,28)\n",
    "plot_image(reimg)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess,'saver/CAE_moedl.ckpt')\n",
    "    img = sess.run(de_data,feed_dict={x:mnist.train.images[0:20], keep_prob: 1.})\n",
    "    reimg = img[0].reshape(28,28)\n",
    "    plot_image(reimg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess,'saver/CAE_moedl.ckpt')\n",
    "    for  i in range(2570):\n",
    "        en_data_ = sess.run(en_data,feed_dict={x:mnist.train.images[i*batch_size:(i+1)*batch_size], keep_prob: 1.})\n",
    "        en_data_ = tf.reshape(en_data_ , [batch_size, 4*4*128])\n",
    "        if i == 0:\n",
    "            data = np.array(en_data_)\n",
    "        else:\n",
    "            data = np.vstack([data,en_data_])\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n",
    "    \"\"\"\n",
    "    Creates a list of random minibatches from (X, Y)\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (input size, number of examples) (m, Hi, Wi, Ci)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples) (m, n_y)\n",
    "    mini_batch_size - size of the mini-batches, integer\n",
    "    seed -- this is only for the purpose of grading, so that you're \"random minibatches are the same as ours.\n",
    "    \n",
    "    Returns:\n",
    "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[0]                  # number of training examples\n",
    "    mini_batches = []\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[permutation,:,:,:]\n",
    "    shuffled_Y = Y[permutation,:]\n",
    "\n",
    "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[k * mini_batch_size : k * mini_batch_size + mini_batch_size,:,:,:]\n",
    "        mini_batch_Y = shuffled_Y[k * mini_batch_size : k * mini_batch_size + mini_batch_size,:]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    # Handling the end case (last mini-batch < mini_batch_size)\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[num_complete_minibatches * mini_batch_size : m,:,:,:]\n",
    "        mini_batch_Y = shuffled_Y[num_complete_minibatches * mini_batch_size : m,:]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'wd1': tf.Variable(tf.random_normal([4*4*128, 1024])), \n",
    "    'wd2': tf.Variable(tf.random_normal([1024, 1024])),\n",
    "    'out': tf.Variable(tf.random_normal([1024, 10]))\n",
    "}\n",
    "biases = {\n",
    "    'bd1': tf.Variable(tf.random_normal([1024])),\n",
    "    'bd2': tf.Variable(tf.random_normal([1024])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tf Graph input\n",
    "x = tf.placeholder(tf.float32, [None, 2048])\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "keep_prob = tf.placeholder(tf.float32) # dropout (keep probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = customnet(_X, _weights, _biases, _dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    # Keep training until reach max iterations\n",
    "    for step in range(1,100):\n",
    "        minibatches = random_mini_batches(data, mnist.train.labels, minibatch_size = 20, 1)\n",
    "        for minibatch in minibatches:\n",
    "            (batch_xs, batch_ys) = minibatch\n",
    "            sess.run(optimizer, feed_dict={x: batch_xs, y: batch_ys, keep_prob: dropout})\n",
    "        if step % 10 == 0:\n",
    "            # Calculate batch accuracy\n",
    "            acc = sess.run(accuracy, feed_dict={x: batch_xs, y: batch_ys, keep_prob: 1.})\n",
    "            # Calculate batch loss\n",
    "            loss = sess.run(cost, feed_dict={x: batch_xs, y: batch_ys, keep_prob: 1.})\n",
    "            print (\"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \"{:.5f}\".format(acc))\n",
    "    saver.save(sess,'saver/CAE_class_moedl.ckpt')      \n",
    "    print (\"Optimization Finished!\")\n",
    "    # Calculate accuracy for 256 mnist test images\n",
    "    print (\"Testing Accuracy:\", sess.run(accuracy, feed_dict={x: mnist.test.images[:256], y: mnist.test.labels[:256], keep_prob: 1.}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\RUANJIAN\\Anaconda3\\Anaconda3_3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting E:\\python\\mnist_data\\train-images-idx3-ubyte.gz\n",
      "Extracting E:\\python\\mnist_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting E:\\python\\mnist_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting E:\\python\\mnist_data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from  tensorflow.examples.tutorials.mnist import  input_data\n",
    "mnist = input_data.read_data_sets(r'E:\\python\\mnist_data', one_hot=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "lr = 0.001                  # learning rate\n",
    "training_iters = 10000     # train step 上限\n",
    "batch_size = 128            \n",
    "n_inputs = 28               # MNIST data input (img shape: 28*28)\n",
    "n_steps = 28                # time steps\n",
    "n_hidden_units = 128        # neurons in hidden layer\n",
    "n_classes = 10              # MNIST classes (0-9 digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# x y placeholder\n",
    "x = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "# 对 weights biases 初始值的定义\n",
    "weights = {\n",
    "    # shape (28, 128)\n",
    "    'w_in': tf.Variable(tf.random_normal([n_inputs, n_hidden_units])),\n",
    "    # shape (128, 10)\n",
    "    'w_out': tf.Variable(tf.random_normal([n_hidden_units, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    # shape (128, )\n",
    "    'b_in': tf.Variable(tf.constant(0.1, shape=[n_hidden_units, ])),\n",
    "    # shape (10, )\n",
    "    'b_out': tf.Variable(tf.constant(0.1, shape=[n_classes, ]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RNN(X, weights, biases):\n",
    "    # 原始的 X 是 3 维数据, 我们需要把它变成 2 维数据才能使用 weights 的矩阵乘法\n",
    "    # X ==> (128 batches * 28 steps, 28 inputs)\n",
    "    X = tf.reshape(X, [-1, n_inputs])\n",
    "\n",
    "    # X_in = W*X + b\n",
    "    X_in = tf.matmul(X, weights['w_in']) + biases['b_in']\n",
    "    # X_in ==> (128 batches, 28 steps, 128 hidden) 换回3维\n",
    "    X_in = tf.reshape(X_in, [-1, n_steps, n_hidden_units])\n",
    "    \n",
    "    # 使用 basic LSTM Cell.\n",
    "    lstm_cell = tf.contrib.rnn.BasicLSTMCell(n_hidden_units, forget_bias=1.0, state_is_tuple=True)\n",
    "    init_state = lstm_cell.zero_state(batch_size, dtype=tf.float32) # 初始化全零 state\n",
    "    \n",
    "    outputs, final_state = tf.nn.dynamic_rnn(lstm_cell, X_in, initial_state=init_state, time_major=False)\n",
    "    #results = tf.matmul(final_state[1],weights['w_out']) + biases['b_out']\n",
    "    # 把 outputs 变成 列表 [(batch, outputs)..] * steps\n",
    "    outputs = tf.unstack(tf.transpose(outputs, [1,0,2]))\n",
    "    results = tf.matmul(outputs[-1], weights['out']) + biases['out']    #选取最后一个 output\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred  = RNN(x,weights,biases)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred,labels=y))\n",
    "train_op = tf.train.AdamOptimizer(lr).minimize(cost)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct_pred = tf.equal(tf.argmax(pred,1),tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred,tf.float32))\n",
    "init  = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.109375\n",
      "0.796875\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    step = 0\n",
    "    while step * batch_size < training_iters:\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        batch_xs = batch_xs.reshape([batch_size, n_steps, n_inputs])\n",
    "        sess.run([train_op], feed_dict={\n",
    "            x: batch_xs,\n",
    "            y: batch_ys,\n",
    "        })\n",
    "        if step % 50 == 0:\n",
    "            print(sess.run(accuracy, feed_dict={\n",
    "            x: batch_xs,\n",
    "            y: batch_ys,\n",
    "        }))\n",
    "        step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
