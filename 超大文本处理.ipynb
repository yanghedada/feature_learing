{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read_csv中有个参数chunksize，\n",
    "\n",
    "通过指定一个chunksize分块大小来读取文件，\n",
    "\n",
    "返回的是一个可迭代的对象TextFileReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "reader = pd.read_csv('tmp.csv', chunksize=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for chunk in reader:\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "指定iterator=True 也可以返回一个可迭代对象TextFileReader ："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reader = pd.read_table('tmp.sv', sep='|', iterator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reader.get_chunk(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我需要打开的数据集是个csv文件，大小为3.7G，\n",
    "\n",
    "并且对于数据一无所知，所以首先打开前5行观察数据的类型，列标签等等："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chunks = pd.read_csv('train.csv',iterator = True)\n",
    "chunk = chunks.get_chunk(5)\n",
    "print (chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preliminary\n",
    "　　我们谈到“文本处理”时，我们通常是指处理的内容。Python 将文本文件的内容读入可以操作的字符串变量非常容易。文件对象提供了三个“读”方法： .read()、.readline() 和 .readlines()。每种方法可以接受一个变量以限制每次读取的数据量，但它们通常不使用变量。 .read() 每次读取整个文件，它通常用于将文件内容放到一个字符串变量中。然而.read() 生成文件内容最直接的字符串表示，但对于连续的面向行的处理，它却是不必要的，并且如果文件大于可用内存，则不可能实现这种处理。下面是read()方法示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    f = open('/path/to/file', 'r')\n",
    "    print f.read()\n",
    "finally:\n",
    "    if f:\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "调用read()会一次性读取文件的全部内容，如果文件有10G，内存就爆了，所以，要保险起见，可以反复调用read(size)方法，每次最多读取size个字节的内容。另外，调用readline()可以每次读取一行内容，调用readlines()一次读取所有内容并按行返回list。因此，要根据需要决定怎么调用。\n",
    "　　如果文件很小，read()一次性读取最方便；如果不能确定文件大小，反复调用read(size)比较保险；如果是配置文件，调用readlines()最方便："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for line in f.readlines():\n",
    "    process(line) # <do something with line>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read In Chunks\n",
    "　　处理大文件是很容易想到的就是将大文件分割成若干小文件处理，处理完每个小文件后释放该部分内存。这里用了iter & yield\n",
    "  \n",
    "  指定每次读取的长度\n",
    "\n",
    "有时，可能希望对每次读取的内容进行更细粒度的控制。\n",
    "\n",
    "在这种情况下，可以使用 iter 和 yield"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_in_chunks(filePath, chunk_size=1024*1024):\n",
    "    \"\"\"\n",
    "    Lazy function (generator) to read a file piece by piece.\n",
    "    Default chunk size: 1M\n",
    "    You can set your own chunk size\n",
    "    \"\"\"\n",
    "    file_object = open(filePath)\n",
    "    while True:\n",
    "        chunk_data = file_object.read(chunk_size)\n",
    "        if not chunk_data:\n",
    "            break\n",
    "        yield chunk_data\n",
    "\n",
    "filePath = './path/filename'\n",
    "for chunk in read_in_chunks(filePath):\n",
    "    process(chunk) # <do something with chunk>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using with open()\n",
    "\n",
    "　　with语句打开和关闭文件，包括抛出一个内部块异常。for line in f文件对象f视为一个迭代器，会自动的采用缓冲IO和内存管理，所以你不必担心大文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#If the file is line based\n",
    "with open(...) as f:\n",
    "    for line in f:\n",
    "    process(line) # <do something with line>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "简述\n",
    "\n",
    "在处理大数据时，有可能会碰到好几个 G 大小的文件。如果通过一些工具（例如：NotePad++）打开它，会发生错误，无法读取任何内容。\n",
    "\n",
    "那么，在 Python 中，如何快速地读取这些大文件呢？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一般的读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('filename', 'r', encoding = 'utf-8') as f:\n",
    "    for line in f.readlines():\n",
    "        do_something(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "但是，当完成这一操作时，readlines() 方法（read() 也一样）会将整个文件加载到内存中。在文件较大时，往往会引发 MemoryError（内存溢出）。\n",
    "\n",
    "那么，如何避免这个问题？\n",
    "\n",
    "使用 fileinput 模块\n",
    "\n",
    "稍微好点儿的方式是使用 fileinput 模块："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import fileinput\n",
    "\n",
    "for line in fileinput.input(['filename']):\n",
    "    do_something(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "调用 fileinput.input() 会按照顺序读取行，但是在读取之后不会将它们保留在内存中。\n",
    "\n",
    "逐行读取\n",
    "\n",
    "除此之外，也可使用 while() 循环和 readline() 来逐行读取："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('filename', 'r', encoding = 'utf-8') as f:\n",
    "    while True:\n",
    "        line = f.readline()  # 逐行读取\n",
    "        if not line:  # 到 EOF，返回空字符串，则终止循环\n",
    "            break\n",
    "        do_something(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自动管理\n",
    "\n",
    "这才是 Pythonci 最完美的方式，既高效又快速\n",
    "\n",
    "with 语句句柄负责打开和关闭文件（包括在内部块中引发异常时），for line in f 将文件对象 f 视为一个可迭代的数据类型，会自动使用 IO 缓存和内存管理，这样就不必担心大文件了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('filename', 'r', encoding = 'utf-8') as f:\n",
    "    for line in f:\n",
    "        do_something(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas提供了IO工具可以将大文件分块读取，测试了一下性能，完整加载9800万条数据也只需要263秒左右，还是相当不错了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "importpandas as pd\n",
    "\n",
    "reader =pd.read_csv('data/servicelogs', iterator=True)\n",
    "\n",
    "try:\n",
    "\n",
    "    df =reader.get_chunk(100000000)\n",
    "\n",
    "exceptStopIteration:\n",
    "\n",
    "    print\"Iteration is stopped.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用不同分块大小来读取再调用 pandas.concat 连接DataFrame，chunkSize设置在1000万条左右速度优化比较明显。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loop =True\n",
    "\n",
    "chunkSize =100000\n",
    "\n",
    "chunks =[]\n",
    "\n",
    "whileloop:\n",
    "\n",
    "    try:\n",
    "\n",
    "        chunk =reader.get_chunk(chunkSize)\n",
    "\n",
    "        chunks.append(chunk)\n",
    "\n",
    "    exceptStopIteration:\n",
    "\n",
    "        loop =False\n",
    "\n",
    "        print\"Iteration is stopped.\"\n",
    "\n",
    "df =pd.concat(chunks, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read_csv()函数的iterator参数等于True时，表示返回一个TextParser以便逐块读取文件；\n",
    "\n",
    "chunkSize表示文件块的大小，用于迭代；\n",
    "\n",
    "TextParser类的get_chunk方法用于读取任意大小的文件块；\n",
    "\n",
    "StopIteration的异常表示在循环对象穷尽所有元素时报错；\n",
    "\n",
    "concat()函数用于将数据做轴向连接：\n",
    "pd.concat(objs, axis=0, join='outer', join_axes=None, ignore_index=False, keys=None, levels=None, names=None, Verify_integrity=False)\n",
    "常用参数： \n",
    "objs：Series,DataFrame或者是Panel构成的序列list; \n",
    "axis：需要合并连接的轴，0是行，1是列； \n",
    "join：连接的参数，inner或outer； \n",
    "ignore=True表示重建索引。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "1. read() 接口的问题\n",
    "\n",
    "f = open(filename, 'rb')\n",
    "f.read()\n",
    "\n",
    "我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执行的操作，是一次性全部读入内存，显然会造成：\n",
    "\n",
    "MemoryError\n",
    "...\n",
    "\n",
    "也即会发生内存溢出。\n",
    "\n",
    "2. 解决方案：转换接口\n",
    "\n",
    "（1）readlines() ：读取全部的行，构成一个 list，实践表明还是会造成内存的问题；\n",
    "\n",
    "for line in f.reanlines():\n",
    "    ...\n",
    "\n",
    "（2）readline()：每次读取一行，\n",
    "\n",
    "while True:\n",
    "    line = f.readline()\n",
    "    if not line:\n",
    "        break\n",
    "\n",
    "（3）read(1024)：重载，指定每次读取的长度\n",
    "\n",
    "while True:\n",
    "    block = f.read(1024)\n",
    "    if not block:\n",
    "\n",
    "3. 真正 Pythonic 的方法\n",
    "\n",
    "真正 Pythonci 的方法，使用 with 结构：\n",
    "\n",
    "with open(filename, 'rb') as f:\n",
    "    for line in f:\n",
    "        <do something with the line>\n",
    "\n",
    "对可迭代对象 f，进行迭代遍历：for line in f，会自动地使用缓冲IO（buffered IO）以及内存管理，而不必担心任何大文件的问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 6, 9]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samp  = [ i for i in sorted(random.sample(range(10),5))]\n",
    "samp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.08155842, 0.80155206, 0.48521373, 0.04952713, 0.99588369,\n",
       "       0.00384505, 0.92578094, 0.577394  , 0.61168731, 0.81880164])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
